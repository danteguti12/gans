{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "musicgan.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZkkJnjX6NB0",
        "colab_type": "text"
      },
      "source": [
        "# Music Composition with Neural Networks\n",
        "*Dante Gutierrez*\n",
        "\n",
        "To do:\n",
        "\n",
        "\n",
        "*   Train ucGAN or Progressive GAN with spectrograms (use padding for vectors)\n",
        "*   Convert MIDI data to multidimensional arrays and train GAN\n",
        "*   Future step: Use embedding and natural language processing with MIDI (molecule analog)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t87728_7rNSt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "afdfd98e-ddad-4c27-a112-2df523dada89"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyBBp0ns3QkX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "c4f217f0-ad9e-4281-9089-6b3b4fa45ff7"
      },
      "source": [
        "!pip install pydub "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydub\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/d1/fbfa79371a8cd9bb15c2e3c480d7e6e340ed5cc55005174e16f48418333a/pydub-0.24.1-py2.py3-none-any.whl\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.24.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-K6wpDj5ere",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import wavfile\n",
        "from pydub import AudioSegment\n",
        "from os import path\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "import scipy.signal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZmdOQmL-x91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!zip -r /content/file.zip /content/wavplots\n",
        "#files.download(\"/content/file.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e5dRd2jroaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wdir = \"/content/drive/My Drive/New Music/Supreme 2/\"\n",
        "odir = \"/content/temp/\"\n",
        "if os.path.exists(odir):\n",
        "    shutil.rmtree(odir)\n",
        "os.mkdir(odir)\n",
        "data = []\n",
        "\n",
        "for file in os.listdir(wdir):\n",
        "  if file.endswith(\".mp3\"):\n",
        "\t\n",
        "    src = wdir + file\n",
        "    dst = odir + os.path.splitext(file)[0] + '.wav'\n",
        "    sound = AudioSegment.from_mp3(src)\n",
        "    sound.export(dst, format=\"wav\")\n",
        "\n",
        "    samplingFreq, signalData = wavfile.read(dst)\n",
        "    signalData = signalData[:,0]\n",
        "\n",
        "    data.append(scipy.signal.spectrogram(signalData, samplingFreq)[2])\n",
        "    #plt.savefig(odir + os.path.splitext(file)[0] + '.png')\n",
        "    os.remove(dst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pac2nYl35FMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_along_axis(array: np.ndarray, target_length: int, axis: int = 0):\n",
        "\n",
        "    pad_size = target_length - array.shape[axis]\n",
        "\n",
        "    if pad_size <= 0:\n",
        "        return array\n",
        "\n",
        "    npad = [(0, 0)] * array.ndim\n",
        "    npad[axis] = (0, pad_size)\n",
        "\n",
        "    return np.pad(array, pad_width=npad, mode='constant', constant_values=0)\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grfM6iIv6hWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_arr = []\n",
        "freq_cap = 20000\n",
        "max_time = 360\n",
        "for x in data:\n",
        "  data_arr.append(pad_along_axis(x[:,:freq_cap], max_time, axis = 0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wELs3Irv-Oia",
        "colab_type": "text"
      },
      "source": [
        "Use pd to obtain quartiles (logarithmical scale) 360 bins *done*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rfcqs12Yx1ZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bins = np.logspace(np.log10(1), np.log10(freq_cap), max_time+1)\n",
        "width = (bins[1:]-bins[:-1])\n",
        "dist = []\n",
        "for sample in data_arr:\n",
        "  temp_sample = []\n",
        "  for row in sample:\n",
        "    temp_sample.append(np.histogram(data_arr[0][0], bins=bins)[0])\n",
        "  dist.append(temp_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDMVRz-3_X6E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "855a091d-20dc-409f-a98d-43690132bc52"
      },
      "source": [
        "print(np.array(dist).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25, 360, 360)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wg9h3GLlyjQu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "47d434ae-74d2-40cd-bc9e-c720693f61e0"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def define_discriminator(in_shape=(max_time,max_time,1)):\n",
        "  model = tf.keras.Sequential([       \n",
        "                               #downsample\n",
        "                               tf.keras.layers.Conv2D(128, (3,3), strides=(2,2), padding='same', input_shape=(360,360,1)),\n",
        "                               tf.keras.layers.LeakyReLU(alpha=0.2),\n",
        "                               #downsample\n",
        "                               tf.keras.layers.Conv2D(128, (3,3), strides=(2,2), padding='same'),\n",
        "                               tf.keras.layers.LeakyReLU(alpha=0.2),\n",
        "                               #classifier\n",
        "                               tf.keras.layers.Flatten(),\n",
        "                               tf.keras.layers.Dropout(0.4),\n",
        "                               tf.keras.layers.Dense(1, activation='sigmoid')])\n",
        "\n",
        "  opt = tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n",
        "  model.compile(\n",
        "      loss='binary_crossentropy', optimizer=opt, metrics=['acc']\n",
        "  )\n",
        "  \n",
        "  return model\n",
        "\n",
        "def define_generator(latent_dim):\n",
        "  n_nodes = 128*90*90\n",
        "  model = tf.keras.Sequential([\n",
        "                              tf.keras.layers.Dense(n_nodes, input_dim=latent_dim),\n",
        "                              tf.keras.layers.LeakyReLU(alpha=0.2),\n",
        "                              tf.keras.layers.Reshape((90, 90, 128)),\n",
        "                              #upsample\n",
        "                              tf.keras.layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'),\n",
        "                              tf.keras.layers.LeakyReLU(alpha=0.2),\n",
        "                              tf.keras.layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'),\n",
        "                              tf.keras.layers.LeakyReLU(alpha=0.2),\n",
        "                              #generate\n",
        "                              tf.keras.layers.Conv2D(1, (90,90), activation='tanh', padding='same')\n",
        "  ])\n",
        "  \n",
        "  return model\n",
        "\n",
        "def define_gan(generator, discriminator):\n",
        "  discriminator.trainable = False\n",
        "  model = tf.keras.Sequential([\n",
        "                               generator,\n",
        "                               discriminator,\n",
        "  ])\n",
        "  opt = tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "\n",
        "  return model\n",
        "\n",
        "print(\"Networks defined\")\n",
        "\n",
        "def load_real_samples(data):\n",
        "  X = np.expand_dims(data, axis=-1)\n",
        "  X = np.float32(X)\n",
        "  X = tf.keras.utils.normalize(X, axis=-1)\n",
        "\n",
        "  return X\n",
        "\n",
        "def generate_real_samples(dataset, n_samples):\n",
        "  ix = np.random.randint(0, dataset.shape[0], n_samples)\n",
        "  X = dataset[ix]\n",
        "  Y = np.ones((n_samples, 1))\n",
        "\n",
        "  return X, Y\n",
        "\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "  x_input = np.random.randn(latent_dim*n_samples)\n",
        "  x_input = x_input.reshape(n_samples, latent_dim)\n",
        "\n",
        "  return x_input\n",
        "\n",
        "def generate_fake_samples(generator, latent_dim, n_samples):\n",
        "  x_input = generate_latent_points(latent_dim, n_samples)\n",
        "  X = generator.predict(x_input)\n",
        "  Y = np.zeros((n_samples, 1))\n",
        "\n",
        "  return X, Y\n",
        "\n",
        "print('Auxilary functions defined')\n",
        "\n",
        "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=15, n_batch=5):\n",
        "  bat_per_epo = int(dataset.shape[0]/n_batch)\n",
        "  half_batch = int(n_batch/2)\n",
        "  for i in range(n_epochs):\n",
        "    for j in range(bat_per_epo):\n",
        "      X_real, Y_real = generate_real_samples(dataset, half_batch)\n",
        "      d_loss1, _ = d_model.train_on_batch(X_real, Y_real)\n",
        "      X_fake, Y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "      d_loss2, _ = d_model.train_on_batch(X_fake, Y_real)\n",
        "      X_gan = generate_latent_points(latent_dim, n_batch)\n",
        "      Y_gan = np.ones((n_batch, 1))\n",
        "      g_loss = gan_model.train_on_batch(X_gan, Y_gan)\n",
        "      print('>%d, %d/%d, d1=%.3f, d2=%.3f, g=%.3f' %\n",
        "            (i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))\n",
        "  g_model.save('generator.h5')\n",
        "\n",
        "print('Training function defined')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Networks defined\n",
            "Auxilary functions defined\n",
            "Training function defined\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVEqWPJ5DTFg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "67c70df5-f4fb-4cda-b991-20f52182e795"
      },
      "source": [
        "latent_dim = 100\n",
        "generator = define_generator(latent_dim)\n",
        "discriminator = define_discriminator()\n",
        "gan_model = define_gan(generator, discriminator)\n",
        "dataset = load_real_samples(dist)\n",
        "train(generator, discriminator, gan_model, dataset, latent_dim)\n",
        "print(\"Training completed\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">1, 1/5, d1=0.716, d2=0.691, g=0.665\n",
            ">1, 2/5, d1=0.000, d2=0.147, g=0.081\n",
            ">1, 3/5, d1=0.000, d2=0.001, g=0.001\n",
            ">1, 4/5, d1=0.000, d2=0.000, g=0.000\n",
            ">1, 5/5, d1=0.000, d2=0.000, g=0.000\n",
            ">2, 1/5, d1=0.000, d2=0.000, g=0.000\n",
            ">2, 2/5, d1=0.000, d2=0.000, g=0.000\n",
            ">2, 3/5, d1=0.000, d2=0.000, g=0.000\n",
            ">2, 4/5, d1=0.000, d2=0.000, g=0.000\n",
            ">2, 5/5, d1=0.000, d2=0.000, g=0.000\n",
            ">3, 1/5, d1=0.000, d2=0.000, g=0.000\n",
            ">3, 2/5, d1=0.000, d2=0.000, g=0.000\n",
            ">3, 3/5, d1=0.000, d2=0.000, g=0.000\n",
            ">3, 4/5, d1=0.000, d2=0.000, g=0.000\n",
            ">3, 5/5, d1=0.000, d2=0.000, g=0.000\n",
            ">4, 1/5, d1=0.000, d2=0.000, g=0.000\n",
            ">4, 2/5, d1=0.000, d2=0.000, g=0.000\n",
            ">4, 3/5, d1=0.000, d2=0.000, g=0.000\n",
            ">4, 4/5, d1=0.000, d2=0.000, g=0.000\n",
            ">4, 5/5, d1=0.000, d2=0.000, g=0.000\n",
            ">5, 1/5, d1=0.000, d2=0.000, g=0.000\n",
            ">5, 2/5, d1=0.000, d2=0.000, g=0.000\n",
            ">5, 3/5, d1=0.000, d2=0.000, g=0.000\n",
            ">5, 4/5, d1=0.000, d2=0.000, g=0.000\n",
            ">5, 5/5, d1=0.000, d2=0.000, g=0.000\n",
            ">6, 1/5, d1=0.000, d2=0.000, g=0.000\n",
            ">6, 2/5, d1=0.000, d2=0.000, g=0.000\n",
            ">6, 3/5, d1=0.000, d2=0.000, g=0.000\n",
            ">6, 4/5, d1=0.000, d2=0.000, g=0.000\n",
            ">6, 5/5, d1=0.000, d2=0.000, g=0.000\n",
            ">7, 1/5, d1=0.000, d2=0.000, g=0.000\n",
            ">7, 2/5, d1=0.000, d2=0.000, g=0.000\n",
            ">7, 3/5, d1=0.000, d2=0.000, g=0.000\n",
            ">7, 4/5, d1=0.000, d2=0.000, g=0.000\n",
            ">7, 5/5, d1=0.000, d2=0.000, g=0.000\n",
            ">8, 1/5, d1=0.000, d2=0.000, g=0.000\n",
            ">8, 2/5, d1=0.000, d2=0.000, g=0.000\n",
            ">8, 3/5, d1=0.000, d2=0.000, g=0.000\n",
            ">8, 4/5, d1=0.000, d2=0.000, g=0.000\n",
            ">8, 5/5, d1=0.000, d2=0.000, g=0.000\n",
            ">9, 1/5, d1=0.000, d2=0.000, g=0.000\n",
            ">9, 2/5, d1=0.000, d2=0.000, g=0.000\n",
            ">9, 3/5, d1=0.000, d2=0.000, g=0.000\n",
            ">9, 4/5, d1=0.000, d2=0.000, g=0.000\n",
            ">9, 5/5, d1=0.000, d2=0.000, g=0.000\n",
            ">10, 1/5, d1=0.000, d2=0.000, g=0.000\n",
            ">10, 2/5, d1=0.000, d2=0.000, g=0.000\n",
            ">10, 3/5, d1=0.000, d2=0.000, g=0.000\n",
            ">10, 4/5, d1=0.000, d2=0.000, g=0.000\n",
            ">10, 5/5, d1=0.000, d2=0.000, g=0.000\n",
            ">11, 1/5, d1=0.000, d2=0.000, g=0.000\n",
            ">11, 2/5, d1=0.000, d2=0.000, g=0.000\n",
            ">11, 3/5, d1=0.000, d2=0.000, g=0.000\n",
            ">11, 4/5, d1=0.000, d2=0.000, g=0.000\n",
            ">11, 5/5, d1=0.000, d2=0.000, g=0.000\n",
            ">12, 1/5, d1=0.000, d2=0.000, g=0.000\n",
            ">12, 2/5, d1=0.000, d2=0.000, g=0.000\n",
            ">12, 3/5, d1=0.000, d2=0.000, g=0.000\n",
            ">12, 4/5, d1=0.000, d2=0.000, g=0.000\n",
            ">12, 5/5, d1=0.000, d2=0.000, g=0.000\n",
            ">13, 1/5, d1=0.000, d2=0.000, g=0.000\n",
            ">13, 2/5, d1=0.000, d2=0.000, g=0.000\n",
            ">13, 3/5, d1=0.000, d2=0.000, g=0.000\n",
            ">13, 4/5, d1=0.000, d2=0.000, g=0.000\n",
            ">13, 5/5, d1=0.000, d2=0.000, g=0.000\n",
            ">14, 1/5, d1=0.000, d2=0.000, g=0.000\n",
            ">14, 2/5, d1=0.000, d2=0.000, g=0.000\n",
            ">14, 3/5, d1=0.000, d2=0.000, g=0.000\n",
            ">14, 4/5, d1=0.000, d2=0.000, g=0.000\n",
            ">14, 5/5, d1=0.000, d2=0.000, g=0.000\n",
            ">15, 1/5, d1=0.000, d2=0.000, g=0.000\n",
            ">15, 2/5, d1=0.000, d2=0.000, g=0.000\n",
            ">15, 3/5, d1=0.000, d2=0.000, g=0.000\n",
            ">15, 4/5, d1=0.000, d2=0.000, g=0.000\n",
            ">15, 5/5, d1=0.000, d2=0.000, g=0.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qypDG1a7dAzF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/generator.h5 /content/drive/My\\ Drive/Colab\\ Notebooks/GANs/musicgan/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}